{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index of contents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Pre-process the reports](#1.-Pre-process-the-reports) <br>\n",
    "2. [Ngram analysis](#2.-Ngram-analysis)<br>\n",
    "3. [Searching for a keyword](#3.-Searching-for-a-keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare the reports to be processed into a csv file with 2 columns - `caseID` and `Report text` and save as `ReportText.csv`\n",
    "2. Pre-process them to remove the stop words, split them into lines and save the `mapping`\n",
    "3. Perform ngram analysis to undestand the most significant words used in the reports\n",
    "4. Use ngram analysis results to find the keyword of the abnormlaity you want\n",
    "5. Understand the negations and their usage in the reports from the ngram analysis\n",
    "6. Use the key word, its negations to find the reports related to the abnormality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-process the reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def split_report_text_to_lines(txt, para_splitter='\\r\\n', line_splitter='.'):\n",
    "    paras = txt.split(para_splitter)\n",
    "    paras = list(filter(None, paras))\n",
    "    \n",
    "    # split the paragraphs to lines\n",
    "    lines = []\n",
    "    for p in paras:\n",
    "        lines += p.split(line_splitter)\n",
    "    \n",
    "    # remove blank lines\n",
    "    if ' ' in lines:\n",
    "        lines.remove(' ')\n",
    "    lines = list(filter(None, lines))\n",
    "    \n",
    "    return lines\n",
    "\n",
    "# load the reports csv/excel file (use pd.read_excel if its an excel file)\n",
    "reports = pd.read_csv('ReportText.csv')\n",
    "patient_ids = np.array(reports.iloc[:,0])\n",
    "report_texts = np.array(reports.iloc[:,1])\n",
    "\n",
    "# make a mapping of all preprocessed report texts\n",
    "mapping = {}\n",
    "for i,pat_id in enumerate(patient_ids):\n",
    "    lines = split_report_text_to_lines(report_texts[i])\n",
    "    \n",
    "    # checking if the report text is empty\n",
    "    if len(lines) > 1:\n",
    "        # removing the first line - unnecessary line\n",
    "        lines = lines[1:]\n",
    "        \n",
    "        # converting all the text to lower case\n",
    "        lines = [l.lower() for l in lines]\n",
    "        \n",
    "        mapping[pat_id] = lines\n",
    "np.save('mapping.npy', mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ngram analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "    \n",
    "# Remove the unnecessary words from a line\n",
    "def remove_words(l):\n",
    "    words_to_remove = ['left', 'right', 'middle', 'lobe', 'lobes', \n",
    "                       'upper', 'lower', 'bilateral', \n",
    "                        '(', ')', ':',';','>','<','``','&', \n",
    "                        'results', 'impression', 'adv', 'conclusion', 'seen', 'small', 'large', 'mediastinum', 'mediastinal', 'trachea', 'bronchi', 'main', 'taken']\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    for w in words_to_remove:\n",
    "        l = l.replace(w,'')\n",
    "    \n",
    "    # toeknize the words\n",
    "    words = word_tokenize(l)\n",
    "    \n",
    "    # remove stop words\n",
    "    words = [w for w in words if w not in stopWords]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Retrn uni, bi, tri and quad grams\n",
    "def get_ngrams(lines):\n",
    "    # Initiate the counters \n",
    "    ug_frequencies, bg_frequencies = Counter([]), Counter([])\n",
    "    tg_frequencies, qg_frequencies = Counter([]), Counter([])\n",
    "    \n",
    "    # find ngrams for each line and append\n",
    "    for l in lines:\n",
    "        l = l.replace(',', ' ')\n",
    "        # remove unneccessary words\n",
    "        words = remove_words(l)\n",
    "        \n",
    "        ug, bg = ngrams(words,1), ngrams(words,2)\n",
    "        tg, qg = ngrams(words,3), ngrams(words,4)\n",
    "        ug_frequencies += Counter(ug)\n",
    "        bg_frequencies += Counter(bg)\n",
    "        tg_frequencies += Counter(tg)\n",
    "        qg_frequencies += Counter(qg)\n",
    "    \n",
    "    return ug_frequencies, bg_frequencies, tg_frequencies, qg_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the preprocessed mapping\n",
    "mapping = np.load('mapping.npy', allow_pickle=True)[()]\n",
    "\n",
    "# collate lines of all reports\n",
    "all_lines = []\n",
    "for patient_id in mapping:\n",
    "    all_lines += mapping[patient_id]\n",
    "\n",
    "unigram_freq, bigram_freq, trigram_freq, quadgram_freq = get_ngrams(all_lines)\n",
    "\n",
    "# Get the most common frequencies\n",
    "ug = np.array(unigram_freq.most_common(100))\n",
    "bg = np.array(bigram_freq.most_common(100))\n",
    "tg = np.array(trigram_freq.most_common(100))\n",
    "qg = np.array(quadgram_freq.most_common(100))\n",
    "\n",
    "df = pd.DataFrame({'Unigrams':ug[:,0], 'Unigram frequencies':ug[:,1], \n",
    "                    'Bigrams':bg[:,0], 'Bigram frequencies':bg[:,1], \n",
    "                    'Trigrams':tg[:,0], 'Trigram frequencies':tg[:,1], \n",
    "                    'Quadgrams':qg[:,0], 'Quadgram frequencies':qg[:,1]})\n",
    "df = df[['Unigrams','Unigram frequencies','Bigrams','Bigram frequencies',\n",
    "         'Trigrams','Trigram frequencies','Quadgrams','Quadgram frequencies']]\n",
    "df.to_excel('ngramAnalysis.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the `ngramAnalysis.xlsx` and understand the occurance of various keywords, their negations etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Searching for a keyword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_presence(lines, ORsearchTags, negations, ANDsearchTags = None):\n",
    "    presence = False   # indicative of that word's presence\n",
    "    for l in lines:\n",
    "        # OR condition on searchtags\n",
    "        cond = False\n",
    "        for word in ORsearchTags:\n",
    "            if word in l:\n",
    "                cond = True\n",
    "                break\n",
    "        \n",
    "        # AND condition on searchtags\n",
    "        if ANDsearchTags is not None:\n",
    "            for word in ANDsearchTags:\n",
    "                if word not in l:cond = False\n",
    "        \n",
    "        # Check for negations if above two are true\n",
    "        if cond:\n",
    "            for word in negations:\n",
    "                if word in l:\n",
    "                    cond = False\n",
    "                    break\n",
    "                    \n",
    "        # if true, set presense to True and break\n",
    "        if cond:\n",
    "            presence = True\n",
    "            break\n",
    "    return presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases with fibrosis -  2631\n"
     ]
    }
   ],
   "source": [
    "mapping = np.load(\"mapping.npy\", allow_pickle=True)[()]\n",
    "negations = [\" no \", \" not \", \" normal \", \" unremarkable \",\n",
    "            \" clear \", \" absent \", \" absence \"]\n",
    "fibrosis_cases = []\n",
    "for pat_id in mapping:\n",
    "    # checking for ground glass haze\n",
    "    cond1 = check_presence(\n",
    "        mapping[pat_id], \n",
    "        ORsearchTags=[\"patchy\", \"haz\"], \n",
    "        negations=negations, \n",
    "        ANDsearchTags=[\"ground glass\"]\n",
    "    )\n",
    "    \n",
    "    # checking for traction bronchiectasis\n",
    "    cond2 = check_presence(\n",
    "        mapping[pat_id],\n",
    "        ORsearchTags=[\"traction\", \"dilatation\"],\n",
    "        negations=negations,\n",
    "        ANDsearchTags=[\"bronchi\"],\n",
    "    )\n",
    "    \n",
    "    # checking for reticulations\n",
    "    cond3 = check_presence(mapping[pat_id], [\"reticula\"], negations)\n",
    "    \n",
    "    # checking for direct reference to fibrosis\n",
    "    cond4 = check_presence(mapping[pat_id], [\"fibrotic\", \"fibrosis\"], negations)\n",
    "\n",
    "    fibrosis_presence = cond1 or cond2 or cond3 or cond4\n",
    "    if fibrosis_presence:\n",
    "        fibrosis_cases.append(pat_id)\n",
    "print(\"Number of cases with fibrosis - \", len(fibrosis_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Patient ID's with fibrosis\": fibrosis_cases})\n",
    "df.to_csv('fibrosis_IDs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
